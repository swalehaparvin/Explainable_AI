{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzsi9iMIX4aqa3/PGN/GPs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swalehaparvin/Explainable_AI/blob/main/Feature_Importance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "Build a decision tree classifier to classify income levels based on multiple features including age, education level, and hours worked per week, and extract the learned rules that explain the decision. Then, compare its performance with an MLPClassifier trained on the same data."
      ],
      "metadata": {
        "id": "pJdsgVkrmoaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn numpy\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "for18GQgotdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: We'll use the Adult Income Dataset (from UCI ML Repository), which contains features like age, education, hours-per-week, and predicts whether income exceeds <=50K or >50K"
      ],
      "metadata": {
        "id": "kY9PyB-9o9Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Load dataset (download from UCI or use fetch_openml)\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
        "           'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',\n",
        "           'hours-per-week', 'native-country', 'income']\n",
        "data = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
        "\n",
        "# Drop missing values and irrelevant columns\n",
        "data.dropna(inplace=True)\n",
        "data.drop(['fnlwgt', 'education-num', 'native-country'], axis=1, inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
        "data_encoded = pd.get_dummies(data, columns=categorical_cols)\n",
        "\n",
        "# Encode target variable (income)\n",
        "label_encoder = LabelEncoder()\n",
        "data_encoded['income'] = label_encoder.fit_transform(data_encoded['income'])\n",
        "\n",
        "# Split into features (X) and target (y)\n",
        "X = data_encoded.drop('income', axis=1)\n",
        "y = data_encoded['income']\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize numerical features (important for MLP)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "X6CxqLWkpAz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Train a Decision Tree and Extract Rules"
      ],
      "metadata": {
        "id": "wQpfpYFzpmcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "\n",
        "# Train Decision Tree\n",
        "dt = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Extract decision rules\n",
        "tree_rules = export_text(dt, feature_names=list(X.columns))\n",
        "print(\"Decision Tree Rules:\\n\", tree_rules)\n",
        "\n",
        "# Evaluate performance\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "print(\"\\nDecision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(classification_report(y_test, y_pred_dt))"
      ],
      "metadata": {
        "id": "ZFrQ5X3ipp5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Train an MLPClassifier and Compare"
      ],
      "metadata": {
        "id": "I6LD4hYSpwr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Train MLP\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate MLP\n",
        "y_pred_mlp = mlp.predict(X_test_scaled)\n",
        "print(\"\\nMLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
        "print(classification_report(y_test, y_pred_mlp))"
      ],
      "metadata": {
        "id": "P_2EF0RtpyFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways\n",
        "\n",
        "### **Decision Tree vs. MLP (Multilayer Perceptron) Classifier**  \n",
        "\n",
        "Both **Decision Trees** and **MLPs (a type of Neural Network)** are supervised learning algorithms, but they differ significantly in structure, interpretability, and use cases.  \n",
        "\n",
        "---\n",
        "\n",
        "## **1. Decision Tree Classifier**  \n",
        "### **How It Works**  \n",
        "- Splits data into branches based on feature values (using criteria like Gini impurity or entropy).  \n",
        "- Forms a tree-like structure of decisions until reaching leaf nodes (predictions).  \n",
        "\n",
        "### **Pros**  \n",
        "‚úÖ **Interpretable** ‚Äì Easy to visualize and explain (unlike neural networks).  \n",
        "‚úÖ **Handles non-linear data** ‚Äì No need for feature scaling.  \n",
        "‚úÖ **Works well with small datasets** ‚Äì Less prone to overfitting than MLPs on small data.  \n",
        "‚úÖ **Handles mixed data types** ‚Äì Works with numerical and categorical features.  \n",
        "\n",
        "### **Cons**  \n",
        "‚ùå **Prone to overfitting** ‚Äì Deep trees memorize noise (requires pruning or ensembling like Random Forest).  \n",
        "‚ùå **Unstable** ‚Äì Small data changes can alter tree structure.  \n",
        "‚ùå **Struggles with complex patterns** ‚Äì May not capture intricate relationships as well as MLPs.  \n",
        "\n",
        "### **When to Use?**  \n",
        "‚úîÔ∏è Need a **simple, explainable model** (e.g., business rules, regulatory compliance).  \n",
        "‚úîÔ∏è **Small to medium datasets** where deep learning would overfit.  \n",
        "‚úîÔ∏è **Non-linear relationships** but not highly complex patterns.  \n",
        "\n",
        "---\n",
        "\n",
        "## **2. MLP (Multilayer Perceptron) Classifier**  \n",
        "### **How It Works**  \n",
        "- A **neural network** with input, hidden, and output layers.  \n",
        "- Uses **backpropagation** and gradient descent to optimize weights.  \n",
        "- Applies **activation functions** (ReLU, sigmoid, tanh) for non-linearity.  \n",
        "\n",
        "### **Pros**  \n",
        "‚úÖ **Handles complex patterns** ‚Äì Can model highly non-linear relationships.  \n",
        "‚úÖ **Works well with large datasets** ‚Äì Improves performance with more data.  \n",
        "‚úÖ **Feature learning** ‚Äì Automatically extracts useful features (unlike decision trees).  \n",
        "\n",
        "### **Cons**  \n",
        "‚ùå **Black-box model** ‚Äì Hard to interpret (not suitable for explainable AI).  \n",
        "‚ùå **Requires feature scaling** ‚Äì Sensitive to input ranges (e.g., StandardScaler needed).  \n",
        "‚ùå **Computationally expensive** ‚Äì Slower training than decision trees.  \n",
        "‚ùå **Hyperparameter-sensitive** ‚Äì Needs tuning (layers, neurons, learning rate).  \n",
        "\n",
        "### **When to Use?**  \n",
        "‚úîÔ∏è **Large datasets** where deep learning excels.  \n",
        "‚úîÔ∏è **High-dimensional data** (e.g., images, text) where feature extraction matters.  \n",
        "‚úîÔ∏è **Complex decision boundaries** that trees struggle with.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Comparison Summary**  \n",
        "\n",
        "| **Factor**            | **Decision Tree** | **MLP (Neural Network)** |\n",
        "|-----------------------|------------------|------------------------|\n",
        "| **Interpretability**  | ‚úÖ High          | ‚ùå Low (Black-box)     |\n",
        "| **Handles Non-linearity** | ‚úÖ Yes | ‚úÖ Yes (Better) |\n",
        "| **Feature Scaling Needed?** | ‚ùå No | ‚úÖ Yes |\n",
        "| **Works with Small Data?** | ‚úÖ Yes | ‚ùå No (Overfits) |\n",
        "| **Training Speed** | ‚ö° Fast | üê¢ Slow (GPU helps) |\n",
        "| **Overfitting Risk** | High (needs pruning) | Medium (needs regularization) |\n",
        "| **Best for Tabular Data?** | ‚úÖ Yes | ‚ö†Ô∏è Depends (often worse than trees/ensembles) |\n",
        "| **Best for Images/Text?** | ‚ùå No | ‚úÖ Yes |\n",
        "\n",
        "---\n",
        "\n",
        "## **Which One to Choose?**  \n",
        "\n",
        "### **Use Decision Tree (or Random Forest) if:**  \n",
        "- You need **explainability** (e.g., business decisions).  \n",
        "- Dataset is **small or medium-sized**.  \n",
        "- Data is **tabular** (structured, like CSV files).  \n",
        "\n",
        "### **Use MLP (Neural Network) if:**  \n",
        "- You have **large amounts of data**.  \n",
        "- Problem involves **complex patterns** (e.g., image recognition, NLP).  \n",
        "- **Feature engineering is difficult** (MLPs learn features automatically).  \n",
        "\n",
        "### **Hybrid Approach?**  \n",
        "- For tabular data, **tree-based models (Random Forest, XGBoost)** often outperform MLPs.  \n",
        "- For unstructured data (images, text), **deep learning (MLP, CNN, RNN)** is better.  \n",
        "\n",
        "Would you like a code example comparing the two on a specific dataset? üöÄ"
      ],
      "metadata": {
        "id": "Q0k3gwCDqjEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing feature impact with linear regression\n",
        "\n",
        "The task is to build and explain a linear regression model that estimates insurance charges based on features like age, BMI, and smoking status by analyzing the model's coefficients to determine the impact of each feature on the predictions."
      ],
      "metadata": {
        "id": "p10StV_Jt006"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NmS6oNMKvWcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import kagglehub\n",
        "\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"mirichoi0218/insurance\")\n",
        "\n",
        "# Find the CSV file (there should be only one)\n",
        "import os\n",
        "\n",
        "# List all files in the downloaded folder\n",
        "files = os.listdir(path)\n",
        "print(\"Files in dataset:\", files)\n",
        "\n",
        "# Load the CSV file (assuming it's called insurance.csv)\n",
        "csv_path = os.path.join(path, \"insurance.csv\")\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Preview the dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "cUusB9EqxQwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Assuming 'df' from cell cUusB9EqxQwO is available\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop('charges', axis=1)\n",
        "y = df['charges']\n",
        "\n",
        "# Preprocess: Scale numeric & encode categorical\n",
        "numeric_features = ['age', 'bmi']\n",
        "categorical_features = ['smoker', 'sex', 'region']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Added train-test split\n",
        "\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test) # Transform test data using the scaler fitted on training data"
      ],
      "metadata": {
        "id": "-wujuK-gu5ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative loading method\n",
        "from sklearn.datasets import fetch_openml\n",
        "data = fetch_openml(name='insurance', version=1, as_frame=True).frame"
      ],
      "metadata": {
        "id": "Q12AM3TIHvlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all available columns\n",
        "print(\"All columns in data:\", data.columns.tolist())\n",
        "\n",
        "# Verify specific columns exist\n",
        "print(\"BMI column exists?\", 'bmi' in data.columns)"
      ],
      "metadata": {
        "id": "sggACrAuH0gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the training data X_train.\n",
        "\n",
        "Fit the linear regression model to the standardized training data.\n",
        "\n",
        "Extract the coefficients from the model.\n",
        "\n",
        "Plot the coefficients for the given feature_names."
      ],
      "metadata": {
        "id": "alR-xSMLuPMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Derive coefficients\n",
        "coefficients = model.coef_\n",
        "\n",
        "# Get feature names after one-hot encoding\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Plot coefficients\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_names, coefficients)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Coefficient Value')\n",
        "plt.title('Feature Impact on Insurance Charges')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "53KYbzAwuNXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze the model to determine the relevant factors influencing smoking status, helping the company assess risk more accurately and tailor insurance policies accordingly."
      ],
      "metadata": {
        "id": "yjzZsNHpwSXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scale the features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Use Linear Regression for continuous target\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)  # y_train should be 'charges'\n",
        "\n",
        "# Get coefficients\n",
        "coefficients = model.coef_\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "# Plot coefficients\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_names, coefficients)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Coefficient Value')\n",
        "plt.title('Feature Impact on Insurance Charges')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F_8JEgMKxUxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing feature importance with decision trees"
      ],
      "metadata": {
        "id": "GQmiwPgEzPAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier # Import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create and train the decision tree\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train) # Use X_train and y_train from the previous cell\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred) # Use y_test from the previous cell\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot feature importance\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "feature_importance = model.feature_importances_\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_names, feature_importance)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Feature Importance')\n",
        "plt.title('Feature Importance in Decision Tree')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oJA9Dr0wy0mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing feature importance with random forests"
      ],
      "metadata": {
        "id": "Rt6WoFgxzxNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = model.feature_importances_\n",
        "\n",
        "# Create generic feature names (adjust the number based on your features)\n",
        "feature_names = [f'Feature_{i}' for i in range(len(feature_importances))]\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(feature_names, feature_importances)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Feature Importance')\n",
        "plt.title('Feature Importance in Random Forest')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DCcLR-c61KqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Differences in Feature Importance: Random Forests vs. Decision Trees vs. Linear Regression**  \n",
        "\n",
        "#### **1. Decision Trees**  \n",
        "- **How it works:**  \n",
        "  - Measures importance based on how much a feature reduces impurity (Gini/entropy for classification, MSE for regression) when splitting data.  \n",
        "  - Importance = (Total impurity reduction by the feature) / (Total impurity reduction by all features).  \n",
        "- **Pros:**  \n",
        "  - Simple and interpretable (easy to visualize in a single tree).  \n",
        "- **Cons:**  \n",
        "  - **Unstable**‚Äîsmall changes in data can lead to very different importance rankings.  \n",
        "  - **Biased toward high-cardinality features** (e.g., continuous variables often appear more important than categorical ones).  \n",
        "\n",
        "#### **2. Random Forests**  \n",
        "- **How it works:**  \n",
        "  - Averages feature importance across **many decision trees**, each trained on random subsets of data and features (bagging).  \n",
        "  - Also considers **out-of-bag (OOB) error**‚Äîif shuffling a feature increases error, it‚Äôs deemed important.  \n",
        "- **Pros:**  \n",
        "  - **More stable and reliable** than single decision trees (reduces overfitting bias).  \n",
        "  - Handles **non-linear relationships** well.  \n",
        "- **Cons:**  \n",
        "  - Can still **overemphasize correlated features** (if two features are similar, their importance may be split).  \n",
        "  - Computationally slower than a single tree.  \n",
        "\n",
        "#### **3. Linear Regression**  \n",
        "- **How it works:**  \n",
        "  - Importance is derived from **coefficient magnitudes** (for standardized features) or **p-values** (statistical significance).  \n",
        "  - Assumes a **linear relationship** between features and target.  \n",
        "- **Pros:**  \n",
        "  - Provides **direct interpretability** (e.g., \"a 1-unit increase in X increases Y by Œ≤\").  \n",
        "  - Works well when relationships are truly linear.  \n",
        "- **Cons:**  \n",
        "  - **Fails with non-linear relationships** (e.g., interactions, thresholds).  \n",
        "  - **Misleading if features are correlated** (multicollinearity inflates variance of coefficients).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Which Method Works Best?**  \n",
        "| **Scenario**                     | **Best Method**               | **Why?** |\n",
        "|----------------------------------|-------------------------------|----------|\n",
        "| **Linear relationships**         | Linear Regression             | Coefficients directly quantify feature impact. |\n",
        "| **Non-linear relationships**     | Random Forest                 | Captures complex interactions; more stable than single trees. |\n",
        "| **Need interpretability**        | Decision Tree (if simple)     | Easy to visualize in a single tree. |\n",
        "| **High-dimensional data**        | Random Forest                 | Handles many features robustly. |\n",
        "| **Correlated features**          | Random Forest (with caution)  | Better than linear regression but may still split importance between correlated features. |\n",
        "| **Statistical inference needed** | Linear Regression (with p-values) | Tests hypotheses about feature significance. |\n",
        "\n",
        "### **Key Takeaways**  \n",
        "- **Random Forests** are generally the **most reliable** for feature importance in real-world data (handles non-linearity, robust to noise).  \n",
        "- **Linear Regression** is best **only if relationships are linear** (and features are uncorrelated).  \n",
        "- **Single Decision Trees** are **unstable**‚Äîuseful for quick insights but not for final decisions.  "
      ],
      "metadata": {
        "id": "Udvsw7uGDVJF"
      }
    }
  ]
}